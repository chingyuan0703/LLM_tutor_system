import ollama
import os
import json
import re
from datetime import datetime
from openpyxl import Workbook, load_workbook
from collections import defaultdict
from langchain_community.document_loaders import PyPDFLoader
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ------------------ æ•™æä¾†æº ------------------

# åˆ—å‡ºæ‰€æœ‰è¦è®€å–ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«çš„æ•™æ PDF æª”å
PDF_LIST = [
    "Abraham Silberschatz Operating System Concepts.pdf",
    "Computer Organization and Design.pdf"
]

# å„²å­˜å‘é‡è³‡æ–™åº«çš„ç›®éŒ„èˆ‡æ•™æç´€éŒ„æ¸…å–®
DB_DIR = "./db"
DB_INFO_FILE = os.path.join(DB_DIR, "dbinfo.json")

# ------------------ æª¢æŸ¥æ˜¯å¦æœ‰æ–°æ•™æ ------------------

# å–å¾—ä¸Šæ¬¡å·²è¼‰å…¥çš„æ•™ææª”åæ¸…å–®
def get_loaded_pdf_list():
    if os.path.exists(DB_INFO_FILE):
        with open(DB_INFO_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# å„²å­˜ç›®å‰æ•™ææª”åæ¸…å–®
def save_loaded_pdf_list(pdf_list):
    os.makedirs(DB_DIR, exist_ok=True)
    with open(DB_INFO_FILE, "w", encoding="utf-8") as f:
        json.dump(pdf_list, f, ensure_ascii=False, indent=2)

# ------------------ è‡ªè¨‚ç« ç¯€åˆ‡å‰²å‡½å¼ + åˆ†æ®µ ------------------

# ä¾ç« ç¯€åç¨±ï¼ˆChapter Xï¼‰åˆ†ç« ï¼Œä¸¦ä½¿ç”¨æ–‡å­—åˆ‡å‰²å™¨ç´°åˆ†å…§å®¹ä»¥é©åˆæ¨¡å‹è™•ç†
def split_by_chapter_and_chunk(documents, chunk_size=1000, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chapters = []
    current_chapter = []
    current_title = ""
    for doc in documents:
        text = doc.page_content
        lines = text.splitlines()
        for line in lines:
            if re.match(r'^\s*(Chapter|CHAPTER)\s+\d+', line):
                if current_chapter:
                    content = "\n".join(current_chapter)
                    chunks = splitter.split_text(content)
                    for chunk in chunks:
                        chapters.append(Document(page_content=chunk, metadata={"chapter": current_title}))
                    current_chapter = []
                current_title = line.strip()
            current_chapter.append(line.strip())
    if current_chapter:
        content = "\n".join(current_chapter)
        chunks = splitter.split_text(content)
        for chunk in chunks:
            chapters.append(Document(page_content=chunk, metadata={"chapter": current_title}))
    return chapters

# ------------------ å»ºç«‹çŸ¥è­˜åº« ------------------

# å¦‚æœæ•™ææ¸…å–®èˆ‡ä¸Šæ¬¡ä¸ä¸€è‡´å°±éœ€è¦é‡å»ºè³‡æ–™åº«
need_refresh = sorted(PDF_LIST) != sorted(get_loaded_pdf_list())

if need_refresh:
    print("ğŸ§  åµæ¸¬åˆ°æœ‰æ–°æ•™æè®Šå‹•ï¼Œé–‹å§‹é‡æ–°å»ºç«‹å‘é‡è³‡æ–™åº«...")
    documents = []

    for pdf_file in PDF_LIST:
        if os.path.exists(pdf_file):
            print(f"ğŸ“˜ è¼‰å…¥æ•™æï¼š{pdf_file}")
            loader = PyPDFLoader(pdf_file)
            loaded_docs = loader.load()
            for doc in loaded_docs:
                doc.metadata["source_file"] = pdf_file  # æ¨™è¨˜æ¯æ®µè³‡æ–™çš„æ•™æä¾†æº
            documents.extend(loaded_docs)
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ°æ•™æï¼š{pdf_file}")

    # ä½¿ç”¨ç« ç¯€åˆ†æ®µèˆ‡æ–‡å­—åˆ‡å‰²é€²è¡Œå…§å®¹æ‹†è§£
    docs = split_by_chapter_and_chunk(documents)
    print(f"âœ… å·²ä¾ç« ç¯€åˆ†æ®µç‚º {len(docs)} ç­†")

    # å»ºç«‹åµŒå…¥å‘é‡èˆ‡å‘é‡è³‡æ–™åº«
    embedding = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma.from_documents(docs, embedding, persist_directory=DB_DIR)
    save_loaded_pdf_list(PDF_LIST)
else:
    print("âœ… æ•™ææœªè®Šå‹•ï¼Œç›´æ¥è¼‰å…¥è³‡æ–™åº«")
    embedding = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma(persist_directory=DB_DIR, embedding_function=embedding)

# å»ºç«‹æª¢ç´¢å™¨ä¾›å¾ŒçºŒå•ç­”æŸ¥è©¢ä½¿ç”¨
retriever = vectordb.as_retriever()

# ------------------ å„²å­˜å°è©±ç´€éŒ„ ------------------

# æ¯æ¬¡ä½¿ç”¨è€…å•ç­”éƒ½å„²å­˜åˆ° Excelï¼ˆUTF-8ï¼‰ä¸­ï¼Œé¿å…äº‚ç¢¼èˆ‡è³‡è¨Šæµå¤±

def save_chat_xlsx(user_msg, bot_reply):
    today_str = datetime.now().strftime("%Y-%m-%d")
    filename = f"chat_log_{today_str}.xlsx"
    time_str = datetime.now().strftime("%H:%M:%S")

    try:
        if os.path.exists(filename):
            wb = load_workbook(filename)
            ws = wb.active
        else:
            wb = Workbook()
            ws = wb.active
            ws.append(["æ™‚é–“", "ä½¿ç”¨è€…", "AI å›è¦†"])

        ws.append([time_str, user_msg, bot_reply])
        wb.save(filename)
    except PermissionError:
        print("âš ï¸ æª”æ¡ˆå¯èƒ½å·²é–‹å•Ÿä¸­ï¼Œè«‹é—œé–‰ Excel å¾Œé‡è©¦ã€‚")

# ------------------ LLM å°è©±è™•ç† ------------------

# å»ºç«‹åˆå§‹æç¤ºèˆ‡æ¨¡å‹è¨­å®š
messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç”¨ç¹é«”ä¸­æ–‡å›ç­”çš„å®¶æ•™è€å¸«ï¼Œè«‹ç”¨ç°¡å–®æ–¹å¼è¬›è§£å•é¡Œã€‚"}]
MODEL_NAME = 'llama3'
last_user = ""
last_reply = ""

# å°è©±ä¸»å‡½å¼

def chat_with_ollama(prompt):
    global last_user, last_reply

    # å˜—è©¦å¾æå•ä¸­è‡ªå‹•æŠ“å‡ºä½¿ç”¨è€…æƒ³æŒ‡å®šå“ªæœ¬æ•™æ
    selected_pdf = None
    for pdf_file in PDF_LIST:
        base = os.path.splitext(os.path.basename(pdf_file))[0].lower()
        if base in prompt.lower():
            selected_pdf = pdf_file
            break

    # åŸ·è¡Œæª¢ç´¢ï¼Œè‹¥æœ‰æ•™ææŒ‡å®šå‰‡éæ¿¾å°æ‡‰æ®µè½
    if selected_pdf:
        print(f"ğŸ¯ åªæœå°‹æ•™æï¼š{selected_pdf}")
        all_docs = retriever.invoke(prompt)
        related_docs = [doc for doc in all_docs if doc.metadata.get("source_file") == selected_pdf][:3]
        if not related_docs:
            related_docs = all_docs[:3]  # è‹¥ç„¡çµæœå‰‡å›é€€
    else:
        related_docs = retriever.invoke(prompt)[:3]

    # æ•´ç†å‡ºç­”æ¡ˆçš„ä¸Šä¸‹æ–‡å¼•ç”¨ä¾†æºèˆ‡æ®µè½
    context_chunks = []
    for doc in related_docs:
        source = doc.metadata.get("source_file", "æœªçŸ¥æ•™æ")
        page = doc.metadata.get("page", "æœªçŸ¥é æ•¸")
        chunk = f"[ä¾†è‡ªï¼š{source} ç¬¬ {page} é ]\n{doc.page_content}"
        context_chunks.append(chunk)

    context_text = "\n---\n".join(context_chunks)

    # åŒ…è£ prompt çµ¦æ¨¡å‹ï¼ˆå«ä¸Šä¸‹æ–‡ï¼‰
    rag_prompt = f"""ä»¥ä¸‹æ˜¯æ•™æå…§å®¹æ‘˜è¦ï¼Œè«‹æ ¹æ“šé€™äº›å…§å®¹ä¾†å›ç­”å•é¡Œï¼š\n\n{context_text}\n\nä½¿ç”¨è€…å•é¡Œï¼š{prompt}\nè«‹ç”¨ç¹é«”ä¸­æ–‡è©³ç´°è§£é‡‹ï¼Œä¸¦èˆ‰ä¾‹å­èªªæ˜ã€‚"""

    messages.append({"role": "user", "content": rag_prompt})

    try:
        response = ollama.chat(model=MODEL_NAME, messages=messages)
        reply = response['message']['content']
        messages.append({"role": "assistant", "content": reply})

        # å„²å­˜å°è©±èˆ‡æ›´æ–°è¨˜æ†¶
        save_chat_xlsx(prompt, reply)
        last_user = prompt
        last_reply = reply

        # æ§åˆ¶æ­·å²è¨Šæ¯æ•¸é‡ï¼Œé¿å…éé•·
        if len(messages) > 20:
            messages.pop(1)

        return reply
    except Exception as e:
        return f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

# ------------------ ä¸»ç¨‹å¼ ------------------

if __name__ == "__main__":
    print("ğŸ“˜ å®¶æ•™ç³»çµ±å·²å•Ÿå‹•ï¼Œè¼¸å…¥ quit,exit,bye é›¢é–‹")

    while True:
        user_input = input("ä½ ï¼š").strip()

        if not user_input:
            continue

        if user_input.lower() in ["quit", "exit", "bye"]:
            print("ğŸ‘‹ å†è¦‹ï¼Œç¥å­¸ç¿’æ„‰å¿«ï¼")
            break

        response = chat_with_ollama(user_input)
        print("å®¶æ•™è€å¸«ï¼š", response)
