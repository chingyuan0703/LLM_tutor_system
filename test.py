import ollama
import os
from openpyxl import Workbook, load_workbook
from datetime import datetime
from langchain_community.document_loaders import PyPDFLoader
from langchain_ollama import OllamaEmbeddings 
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ------------------ è¼‰å…¥ PDF æ•™æä¸¦å»ºç«‹å‘é‡è³‡æ–™åº« ------------------

PDF_PATH = "Abraham Silberschatz Operating System Concepts.pdf"
DB_DIR = "./db"

if not os.path.exists(DB_DIR):
    print("ğŸ§  ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼šè¼‰å…¥ PDF ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«...")
    loader = PyPDFLoader(PDF_PATH)
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.split_documents(documents)

    embedding = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma.from_documents(docs, embedding, persist_directory=DB_DIR)
else:
    print("âœ… å·²è¼‰å…¥æœ¬åœ°è³‡æ–™åº«")
    embedding = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma(persist_directory=DB_DIR, embedding_function=embedding)

retriever = vectordb.as_retriever()

# ------------------ å„²å­˜å°è©±ç´€éŒ„ï¼ˆXLSXï¼‰ ------------------

def save_chat_xlsx(user_msg, bot_reply):
    today_str = datetime.now().strftime("%Y-%m-%d")
    filename = f"chat_log_{today_str}.xlsx"

    time_str = datetime.now().strftime("%H:%M:%S")

    if os.path.exists(filename):
        wb = load_workbook(filename)
        ws = wb.active
    else:
        wb = Workbook()
        ws = wb.active
        ws.append(["æ™‚é–“", "ä½¿ç”¨è€…", "AI å›è¦†"])

    ws.append([time_str, user_msg, bot_reply])
    wb.save(filename)


# ------------------ LLM è¨Šæ¯è™•ç† ------------------

messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç”¨ç¹é«”ä¸­æ–‡å›ç­”çš„å®¶æ•™è€å¸«ï¼Œè«‹ç”¨ç°¡å–®æ–¹å¼è¬›è§£å•é¡Œã€‚"}]
MODEL_NAME = 'llama3'

def chat_with_ollama(prompt):
    # ğŸ” æª¢ç´¢æ•™æå…§å®¹
    related_docs = retriever.invoke(prompt)
    context_text = "\n---\n".join([doc.page_content for doc in related_docs[:3]])

    rag_prompt = f"""ä»¥ä¸‹æ˜¯æ•™æå…§å®¹æ‘˜è¦ï¼Œè«‹æ ¹æ“šé€™äº›å…§å®¹ä¾†å›ç­”å•é¡Œï¼š

{context_text}

ä½¿ç”¨è€…å•é¡Œï¼š{prompt}
è«‹ç”¨ç¹é«”ä¸­æ–‡è©³ç´°è§£é‡‹ï¼Œä¸¦èˆ‰ä¾‹å­èªªæ˜ã€‚"""

    messages.append({"role": "user", "content": rag_prompt})

    try:
        response = ollama.chat(model=MODEL_NAME, messages=messages)
        reply = response['message']['content']
        messages.append({"role": "assistant", "content": reply})

        # âœ… å„²å­˜å°è©±
        save_chat_xlsx(prompt, reply)

        # æ§åˆ¶è¨Šæ¯å †ç©ï¼ˆè¨˜æ†¶é™åˆ¶ï¼‰
        if len(messages) > 20:
            messages.pop(1)

        return reply
    except Exception as e:
        return f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

# ------------------ ä¸»ç¨‹å¼ ------------------

if __name__ == "__main__":
    print("ğŸ“˜ å®¶æ•™ç³»çµ±å·²å•Ÿå‹•ï¼Œè¼¸å…¥ quit é›¢é–‹")

    while True:
        user_input = input("ä½ ï¼š").strip()

        if not user_input:
            continue

        if user_input.lower() in ["quit", "exit", "bye"]:
            print("ğŸ‘‹ å†è¦‹ï¼Œç¥å­¸ç¿’æ„‰å¿«ï¼")
            break

        response = chat_with_ollama(user_input)
        print("å®¶æ•™è€å¸«ï¼š", response)
